{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises Week 12 - Association Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Local imports\n",
    "import sys\n",
    "sys.path.append('./utilities')\n",
    "from load_data import load_market_basket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.\n",
    "\n",
    "The work you do in this exercise will be very useful also in your hand-in.\n",
    "\n",
    "We learned the Apriori algorithm in class. Make your own implementation. \n",
    "\n",
    "We will use the anonymized real-world retail market basket data from: http://fimi.ua.ac.be/data/.\n",
    "\n",
    "This data comes from an anonymous Belgian retail store, and was donated by Tom Brijs from Limburgs Universitair Centrum, Belgium. The original data contains 16,470 different items and 88,162 transactions. You may only work with the top-50 items in terms of occurrence frequency.\n",
    "\n",
    "Your task is to:\n",
    "1. Implement the Apriori algorithm.\n",
    "2. Apply the Apriori algorithm on these data to find association rules with minimum support 0.05 and minimum confidence 0.7. Write down the rules in descending order of confidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the retail data\n",
    "transactions = load_market_basket()\n",
    "\n",
    "def book_example():\n",
    "    return [\n",
    "        [1, 2, 4, 5],\n",
    "        [2, 3, 5],\n",
    "        [1, 2, 4, 5],\n",
    "        [1, 2, 3, 5],\n",
    "        [1, 2, 3, 4, 5], \n",
    "        [2, 3, 4],\n",
    "    ]\n",
    "\n",
    "def filter_transactions(T, k=50):\n",
    "    \"\"\"\n",
    "        Keep only the top k items in the transactions.\n",
    "        Remove transactions that become empty.\n",
    "    \"\"\"\n",
    "    # Count occurences of each item\n",
    "    counts = [0] * 16470\n",
    "    for t in T:\n",
    "        for i in t:\n",
    "            counts[i] += 1\n",
    "\n",
    "    # Sort and select top k\n",
    "    counts = np.array(counts)\n",
    "    order  = np.argsort(counts)[::-1] # reverse the sorted order\n",
    "\n",
    "    indexes_to_keep = order[:k]       # Keep the top k items\n",
    "    index_set = set(indexes_to_keep)  # Convert to python set for efficiency\n",
    "\n",
    "    # Filter transactions\n",
    "    T_new = [t_ for t_ in  [list(filter(lambda i: i in index_set, t)) for t in T]  if t_]\n",
    "    return T_new\n",
    "\n",
    "T = filter_transactions(transactions, k=50)\n",
    "#T = book_example() # You can first use the very small dataset to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny function for generating rules from tuples\n",
    "# Ex: rule((1, 2), (5)) outputs \"(1, 2) => (5)\"\n",
    "rule  = lambda lhs, rhs: \"%s => %s\" % (str(lhs), str(rhs)) # For generating rule strings\n",
    "from itertools import chain, combinations\n",
    "\n",
    "def compute_support(Ck, T, still_applicable=None):\n",
    "    # counts = []\n",
    "    # ### TODO Your code here\n",
    "    # for i in Ck:\n",
    "    #     count_i = 0\n",
    "    #     for j in T:\n",
    "    #         check =  all(item in j for item in i)\n",
    "    #         if check:\n",
    "    #             count_i += 1 \n",
    "    #     counts.append(count_i / len(T))\n",
    "    # ### TODO Your code here\n",
    "    # return counts\n",
    "\n",
    "    if still_applicable is None: still_applicable = [True]* len(T)\n",
    "\n",
    "    counts = {}\n",
    "    for row, t in enumerate(T):\n",
    "        found_any = False\n",
    "        for c in Ck:\n",
    "            if set.issubset(set(c), t):\n",
    "                if c in counts: counts[c] += 1\n",
    "                else: counts[c] =1\n",
    "                found_any = True\n",
    "\n",
    "        still_applicable[row] = found_any\n",
    "    return counts\n",
    "\n",
    "def extend_prefix_tree(Ck_prev):\n",
    "    Ck = set()\n",
    "    ### TODO Your code here\n",
    "    for itemset in Ck_prev:\n",
    "        its1 = tuple(sorted(itemset))\n",
    "        for itemset2 in Ck_prev:\n",
    "            its2 = tuple(sorted(itemset2))\n",
    "            if its1[:-1] == its2[:-1] and its1[-1] < its2[-1]: Ck.add(its1 + its2[-1:])\n",
    "\n",
    "    to_remove = set()\n",
    "    for c in Ck:\n",
    "        for subset in combinations(c, len(c)-1):\n",
    "            if not subset in Ck_prev:\n",
    "                to_remove.add(c)\n",
    "                break\n",
    "    \n",
    "    for c in to_remove:\n",
    "        Ck.remove(c)\n",
    "    ### TODO Your code here\n",
    "    return Ck\n",
    "\n",
    "def powerset(iterable):\n",
    "    ### TODO Your code here\n",
    "    # powerset = []\n",
    "    # for i in iterable:\n",
    "    #     for j in iterable:\n",
    "    #         if i != j:\n",
    "    #             intersection = list(set(i) | set(j))\n",
    "    #             if len(intersection) > len(i) + 1:\n",
    "    #                 continue\n",
    "    #             else:\n",
    "    #                 if intersection not in powerset:\n",
    "    #                     powerset.append(intersection)\n",
    "    # ### TODO Your code here\n",
    "    # return powerset\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(1, len(s)))\n",
    "\n",
    "def apriori_algorithm(T, support=0.05, min_confidence=0.7):\n",
    "    \"\"\"\n",
    "        Apriori algorithm for mining association rules.\n",
    "        Inputs:\n",
    "            T:               A list of lists, each inner list will contiain integer-item-ids. \n",
    "                             Example: T = [[1, 2, 5], [2, 3, 4], [1, 6]]\n",
    "            support:         The proportion of occurences needed to keep itemsets.\n",
    "            min_confidence:  Minimum confidence for the algorithm to output the rule.\n",
    "        \n",
    "        Outputs:\n",
    "            rules:           List of tuples [(rule:str, confidence:float), ... ]\n",
    "                             Example: [(\"(1, 2) => (5)\", 0.84), (\"(3, 4) => (7)\", 0.75)]\n",
    "    \"\"\"\n",
    "    ### TODO Your code here\n",
    "    # C1 = [[x] for x in set(chain(*T))]\n",
    "    \n",
    "    # while(len(C1) != 0):\n",
    "    #     support_list = compute_support(C1, book_example())\n",
    "\n",
    "    #     tmp = []\n",
    "    #     for idx, i in enumerate(support_list):\n",
    "    #         if i >= support:\n",
    "    #             tmp.append(C1[idx])\n",
    "\n",
    "    #     C1 = powerset(tmp)\n",
    "    #     print(\"Ck \", C1)\n",
    "\n",
    "    n = len(T)\n",
    "\n",
    "    itemsets = {}\n",
    "    C1 = set()\n",
    "    for t in T:\n",
    "        for t1 in t: C1.add((t1,))\n",
    "\n",
    "    still_applicable = [True] * n\n",
    "    Ck = C1\n",
    "\n",
    "    k = 1\n",
    "\n",
    "    while Ck:\n",
    "        itemsets[k] = compute_support(Ck, T, still_applicable)\n",
    "        Ck_copy = Ck.copy()\n",
    "        for itemset in Ck:\n",
    "            if itemsets[k][itemset] / n < support:\n",
    "                del itemsets[k][itemset]\n",
    "                Ck_copy.remove(itemset)\n",
    "        \n",
    "        Ck = extend_prefix_tree(Ck_copy)\n",
    "        k += 1\n",
    "    \n",
    "    k = 2\n",
    "    rules = []\n",
    "    while k <= max(itemsets.keys()):\n",
    "        for itemset in itemsets[k].keys():\n",
    "            for rhs in powerset(itemset):\n",
    "                remaining = set(itemset).difference(set(rhs))\n",
    "                lhs = tuple(sorted(remaining))\n",
    "\n",
    "                conf = itemsets[k][itemset] / itemsets[len(lhs)][lhs]\n",
    "                if conf >= min_confidence:\n",
    "                    rules.append((rule(lhs, rhs), conf))\n",
    "        k += 1\n",
    "    ### TODO Your code here\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can try your algorithm. If you want, you could try changing the support and min_confidence to see what happens to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conf.    \t Rule\n",
      " 0.8168% \t (41, 48) => (39,)\n",
      " 0.7681% \t (38, 48) => (39,)\n",
      " 0.7637% \t (41,) => (39,)\n"
     ]
    }
   ],
   "source": [
    "rules = apriori_algorithm(T, support=0.05, min_confidence=0.7)\n",
    "rules = sorted(rules, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"%-8s \\t %s\" % (\"Conf.\", \"Rule\"))\n",
    "for r in rules:\n",
    "    print(\"%7.4f%% \\t %s\" % r[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.\n",
    "\n",
    "We have learned how to mine frequent itemsets and association rules from a transaction database where each transaction consists of a simple set of items. You are asked to propose a framework for mining association rules from transaction data, in which each item in a transaction is associated with an integer number that counts how many times the items appears in the transaction. In a market basket, this count number indicates the number of copies of a product in a customerâ€™s basket. For example, we do not only care whether a customer bought fish or not, but how many pieces of fish they bought.\n",
    "\n",
    "1. Define (extend) the notion of an itemset and an association rule in the case of such data.\n",
    "2. Describe an efficient algorithm that mines itemsets and association rules as defined in Exercise 2. Illustrate the pruning strategies used in your algorithm and explain how they relate to the Apriori principle.\n",
    "3. Extend your implementation of the Apriori algorithm to handle this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO your code here\n",
    "# You can copy pase the code from above and adjust it\n",
    "\n",
    "### TODO your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Consider a dataset of transactions $D$, and let $D'$ be a dataset derived from $D$ by independently deleting items from transactions in $D$. In particular, any item in any transaction in $D$ is deleted with probability $p$.\n",
    "\n",
    "1. Given an itemset $S$, compute its expected support in $D'$ as a function of its support in $D$.\n",
    "2. Compute the probability that an itemset $S$, which is frequent in $D$, is also frequent in $D'$, under the same minimum support threshold.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
